{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parkinson's Disease detection with spectral centroid and spectral rolloff audio features using Random Forest coded without libraries."
      ],
      "metadata": {
        "id": "Z5ho7GJ6Kmyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "ArLOWUM60Uy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "%run \"/content/drive/MyDrive/decisiontree.py\"\n",
        "import librosa\n",
        "from librosa import feature\n",
        "import os\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "_2-n7nYA0YtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the audios with Librosa, and preprocess them using Peak Amplitude Normalization.\n",
        "- load_audio will return all the audios with their sampling rate.\n",
        "- to_positive will turn all values to postive numbers (audio is 1D).\n",
        "- get_max will return the maximum value in each audio array.\n",
        "- peak_amplitude_normalize will preprocess audios with this formula 10 ** (peak / 20) / maximum_value."
      ],
      "metadata": {
        "id": "nqBZdQQZ0csS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path) -> Tuple:  # Loads the audio\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=44100)\n",
        "    return audio, sampling_rate\n",
        "\n",
        "\n",
        "def to_positive(n_array):  # Turn all values to positive values\n",
        "    for i in range(len(n_array)):\n",
        "        if n_array[i] < 0:\n",
        "            n_array[i] = -1 * n_array[i]\n",
        "\n",
        "    return n_array\n",
        "\n",
        "\n",
        "def get_max(n_array):  # Get the maximum value\n",
        "    max_value = n_array[0]\n",
        "    for i in range(1, len(n_array), 1):\n",
        "        if n_array[i] > max_value:\n",
        "            max_value = n_array[i]\n",
        "\n",
        "    return max_value\n",
        "\n",
        "\n",
        "def peak_amplitude_normalize(audio_data, peak=-3.0):  # Calculate a scaling factor based on the specific peak value\n",
        "    n_array = to_positive(audio_data)               # (-3 dB) and multiply the entire audio signal by the scaling factor\n",
        "    maximum_value = get_max(n_array)\n",
        "    scaling = 10 ** (peak / 20) / maximum_value\n",
        "    normalized_audio = audio_data * scaling\n",
        "    return normalized_audio"
      ],
      "metadata": {
        "id": "NeL-EQQK0yt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract spectral centroid and spectral rolloff audio features.\n",
        "- spectral_centroid_rolloff function extract both spectral centroid and spectral rolloff features using Librosa library for each audio."
      ],
      "metadata": {
        "id": "_bH31i4B03oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral_centroid_rolloff() -> Tuple:  # Gets each audio from the dataset, get its status, load the audio, clean\n",
        "    directory = '/content/drive/MyDrive/HYP TRAIN DATA/' # the audio compute the features (Spectral Centroid & Rolloff)\n",
        "    features = []\n",
        "    status = []\n",
        "    # Getting each audio path from the dataset\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            if \"P\" in file_name:\n",
        "                status.append(1)\n",
        "\n",
        "            if \"C\" in file_name:\n",
        "                status.append(0)\n",
        "\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            audio, sampling_rate = load_audio(file_path)\n",
        "            preprocessed_audio = peak_amplitude_normalize(audio)\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=preprocessed_audio, sr=sampling_rate).mean()\n",
        "            spectral_roll_off = librosa.feature.spectral_rolloff(y=preprocessed_audio, sr=sampling_rate).mean()\n",
        "            feats = np.array([spectral_centroid, spectral_roll_off])\n",
        "            features.append(feats)\n",
        "\n",
        "    features = np.array(features)\n",
        "    status = np.array(status)\n",
        "    return features, status"
      ],
      "metadata": {
        "id": "cU4SLCbw1FZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Class Random Forest takes number_trees, maximum_depth, minimum_samples_split, and num_features as data members.\n",
        "- Random Forest uses Decision Tree to build more than one tree, then after each tree makes a prediction and then hold majority vote after that."
      ],
      "metadata": {
        "id": "JHDGhx6M1Lm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:  # Constructor\n",
        "    def __init__(self, number_trees=10, maximum_depth=10, minimum_samples_split=2, num_features=None):\n",
        "        self._number_trees = number_trees\n",
        "        self._maximum_depth = maximum_depth\n",
        "        self._minimum_samples_split = minimum_samples_split\n",
        "        self._num_features = num_features\n",
        "        self._trees = []\n",
        "\n",
        "    def fit(self, x, y):  # Fit method to fit the data\n",
        "        self._trees = []\n",
        "        for _ in range(self._number_trees):\n",
        "            tree = DecisionTree(max_depth=self._maximum_depth, min_samples_split=self._minimum_samples_split,\n",
        "                                n_features=self._num_features)  # Instantiate a Decision Tree object\n",
        "            x_sample, y_sample = self._random_samples(x, y)\n",
        "            tree.fit(x_sample, y_sample)  # Fit the tree\n",
        "            self._trees.append(tree)  # Append the tree to the tree list\n",
        "\n",
        "    def _random_sample_indexes(self, a, size):  # Given a number a choose at random size numbers from 0 to a - 1\n",
        "        if size > a:                            # a not included\n",
        "            raise ValueError(str(size) + \" features can't be chosen out of \" + str(a))\n",
        "\n",
        "        random_sample_indexes = []\n",
        "        for _ in range(size):\n",
        "            random_sample_index = random.randrange(0, a, 1)\n",
        "            random_sample_indexes.append(random_sample_index)\n",
        "\n",
        "        random_sample_indexes = np.array(random_sample_indexes)\n",
        "\n",
        "        return random_sample_indexes\n",
        "\n",
        "    def _random_samples(self, x, y):  # Chooses random samples for each tree\n",
        "        number_samples = x.shape[0]\n",
        "        indexes = self._random_sample_indexes(number_samples, number_samples)\n",
        "        return x[indexes], y[indexes]\n",
        "\n",
        "    def _unique_labels(self, y):  # Given y labels find the unique labels\n",
        "        y = list(y)\n",
        "        label = y[0]\n",
        "        list_labels = []\n",
        "        list_labels.append(label)\n",
        "\n",
        "        for i in range(1, len(y), 1):\n",
        "            label = y[i]\n",
        "            if label in list_labels:\n",
        "                pass\n",
        "            else:\n",
        "                list_labels.append(label)\n",
        "\n",
        "        list_labels = np.array(list_labels)\n",
        "        list_labels.sort()\n",
        "\n",
        "        return list_labels\n",
        "\n",
        "    def _majority_label(self, y):  # Given y labels find and return the most occurring label\n",
        "        labels = self._unique_labels(y)\n",
        "        occurrences = []\n",
        "        majority_lab = y[0]\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            num_occur = 0\n",
        "            for lab in y:\n",
        "                if lab == label:\n",
        "                    num_occur += 1\n",
        "\n",
        "            occurrences.append(num_occur)\n",
        "\n",
        "        for i in range(len(occurrences)):\n",
        "            for n in range(len(occurrences)):\n",
        "                if i == n:\n",
        "                    pass\n",
        "                else:\n",
        "                    if occurrences[i] > occurrences[n]:\n",
        "                        majority_lab = labels[i]\n",
        "\n",
        "        return majority_lab\n",
        "\n",
        "    def predict(self, x):  # Given X data, every tree will make a prediction for each x data\n",
        "        estimations = []\n",
        "        for tree in self._trees:\n",
        "            estimations.append(tree.predict(x))\n",
        "\n",
        "        tree_estimates = []\n",
        "        for i in range(len(x)):\n",
        "            estimate = []\n",
        "            for e in estimations:\n",
        "                estimate.append(e[i])\n",
        "\n",
        "            estimate = np.array(estimate)\n",
        "            tree_estimates.append(estimate)\n",
        "\n",
        "        tree_estimates = np.array(tree_estimates)\n",
        "        tree_predictions = []\n",
        "\n",
        "        for tree_estimate in tree_estimates:\n",
        "            tree_predictions.append(self._majority_label(y=tree_estimate))\n",
        "\n",
        "        tree_predictions = np.array(tree_predictions)\n",
        "        return tree_predictions"
      ],
      "metadata": {
        "id": "aDGl6pBa2EDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Standard scaler and splitting of the data.\n",
        "- standard_scaler uses means and std deviations to standardize the features.\n",
        "- split splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "Bteb_Coa2Mrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(features):\n",
        "    # Computation of mean and standard deviation\n",
        "    means = features.mean(axis=0)\n",
        "    std_deviations = features.std(axis=0)\n",
        "\n",
        "    # Standardizing the features\n",
        "    standardized_features = (features - means) / std_deviations\n",
        "    standardized_features = np.array(standardized_features)\n",
        "\n",
        "    return standardized_features\n",
        "\n",
        "\n",
        "def _random_indexes(array, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > array:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(array))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, array, 1)\n",
        "    random_indexes.append(random_index)\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, array, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, array, 1)\n",
        "\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "\n",
        "    return random_indexes\n",
        "\n",
        "\n",
        "def split(features, targets, test_size, random_state=50):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "6eLTqpqt2Tr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "uc-J5peT2l0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "\n",
        "            else:\n",
        "                false_positive += 1\n",
        "\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "\n",
        "            else:\n",
        "                false_negative += 1\n",
        "\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "-U8xrmFu2t9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Main function."
      ],
      "metadata": {
        "id": "Vnu9j8xt2497"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jhVK4CmHQUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3db55a-cb57-4ef4-a342-a320ecc5fc0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score on the validation data:  0.75\n",
            "Precision score on the validation data:  0.5\n",
            "Recall score on the validation data:  1.0\n",
            "F1 score on the validation data:  0.6666666666666666\n",
            "The confusion matrix on the validation:  [[3, 3], [0, 6]]\n",
            "Accuracy score on the validation data:  0.75\n",
            "Precision score on the validation data:  0.5\n",
            "Recall score on the validation data:  1.0\n",
            "F1 score on the validation data:  0.6666666666666666\n",
            "The confusion matrix on the validation:  [[3, 3], [0, 6]]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    feats, y_labels = spectral_centroid_rolloff()\n",
        "    features = standard_scaler(feats)\n",
        "\n",
        "    X_train, X_valid, y_train, y_valid = split(features, y_labels, test_size=0.20)\n",
        "\n",
        "    rf = RandomForest(minimum_samples_split=2, maximum_depth=1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    predictions = rf.predict(X_valid)\n",
        "\n",
        "    # Metrics on the validation data\n",
        "    accuracy = accuracy_score(y_valid, predictions)\n",
        "    precision = precision_score(y_valid, predictions)\n",
        "    recall = recall_score(y_valid, predictions)\n",
        "    f1 = f1_score(y_valid, predictions)\n",
        "    confusion_mat = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "    print('Accuracy score on the validation data: ', accuracy)\n",
        "    print('Precision score on the validation data: ', precision)\n",
        "    print('Recall score on the validation data: ', recall)\n",
        "    print('F1 score on the validation data: ', f1)\n",
        "    print('The confusion matrix on the validation: ', confusion_mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Vb885Y7oTtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}