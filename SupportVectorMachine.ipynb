{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parkinson's Disease detection with Mel Frequency Cepstral Coefficient (MFCC) audio features using Support Vector Machine (SVM) coded without libraries."
      ],
      "metadata": {
        "id": "bYvV7gCRJTJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "MLjjJKGDqWNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from librosa import feature\n",
        "from typing import Tuple\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "hDQF_ZqVqbw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the audios with Librosa, and preprocess them using Peak Amplitude Normalization.\n",
        "- load_audio will return all the audios with their sampling rate.\n",
        "- to_positive will turn all values to postive numbers (audio is 1D).\n",
        "- get_max will return the maximum value in each audio array.\n",
        "- peak_amplitude_normalize will preprocess audios with this formula 10 ** (peak / 20) / maximum_value."
      ],
      "metadata": {
        "id": "Y2qbsy_Aqnqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path) -> Tuple:\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=44100)\n",
        "    return audio, sampling_rate\n",
        "\n",
        "def to_positive(n_array):  # Turn all values to positive values\n",
        "    for i in range(len(n_array)):\n",
        "        if n_array[i] < 0:\n",
        "            n_array[i] = -1 * n_array[i]\n",
        "    return n_array\n",
        "\n",
        "def get_max(n_array):  # Get the maximum value\n",
        "    max_value = n_array[0]\n",
        "    for i in range(1, len(n_array), 1):\n",
        "        if n_array[i] > max_value:\n",
        "            max_value = n_array[i]\n",
        "    return max_value\n",
        "\n",
        "def peak_amplitude_normalize(audio_data, peak=-3.0):  # Calculate a scaling factor based on the specific peak value\n",
        "    n_array = to_positive(audio_data)               # (-3 dB) and multiply the entire audio signal by the scaling factor\n",
        "    maximum_value = get_max(n_array)\n",
        "    scaling = 10 ** (peak / 20) / maximum_value\n",
        "    normalized_audio = audio_data * scaling\n",
        "    return normalized_audio"
      ],
      "metadata": {
        "id": "IkpFn91Wq3ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract MFCC audio features.\n",
        "- extract_mfcc uses librosa to extract MFCC features (with number of features 13), for each audio."
      ],
      "metadata": {
        "id": "sYZASb_1rBqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc() -> Tuple:\n",
        "    directory = '/content/drive/MyDrive/HYP TRAIN DATA/'\n",
        "    feats = []\n",
        "    status = []\n",
        "    # Getting each audio path from the dataset\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            if \"P\" in file_name:\n",
        "                status.append(1)\n",
        "            if \"C\" in file_name:\n",
        "                status.append(0)\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            audio, sampling_rate = load_audio(file_path)\n",
        "            preprocessed_audio = peak_amplitude_normalize(audio)\n",
        "            mfcc = np.mean(librosa.feature.mfcc(y=preprocessed_audio, sr=sampling_rate, n_mfcc=13).T, axis=0)\n",
        "            feats.append(mfcc)\n",
        "\n",
        "    feats = np.array(feats)\n",
        "    status = np.array(status)\n",
        "    return feats, status"
      ],
      "metadata": {
        "id": "2net03tvrqHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. SVM class with data members learning rate, number of iterations, lambda, weights and bias.\n",
        "- SVM uses a boundary decision (hyperplane) to separate two classes into 1 and -1.\n",
        "- The weights and bias are derived using Gradient Descent."
      ],
      "metadata": {
        "id": "CEXp0MeNrwJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupportVectorMachine:\n",
        "    def __init__(self, learning_rate, num_iterations, lambda_value, weight, bias):\n",
        "        self._learning_rate = learning_rate\n",
        "        self._num_iterations = num_iterations\n",
        "        self._lambda_value = lambda_value\n",
        "        self._weight = weight\n",
        "        self._bias = bias\n",
        "\n",
        "    def return_learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    def get_num_iterations(self):\n",
        "        return self._num_iterations\n",
        "\n",
        "    def get_lambda_value(self):\n",
        "        return self._lambda_value\n",
        "\n",
        "    def update_weight(self, weight):\n",
        "        self._weight = weight\n",
        "\n",
        "    def update_bias(self, bias):\n",
        "        self._bias = bias\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self._weight\n",
        "\n",
        "    def get_bias(self):\n",
        "        return self._bias\n",
        "\n",
        "    def fitting(self, data_points, y_values):\n",
        "        self._rows, self.cols = data_points.shape\n",
        "        # Initializing the weight and bias values\n",
        "        # self._the_weight = np.zeros(self.cols)\n",
        "        # self._the_weight = []\n",
        "        #\n",
        "        # for c in range(self.cols):\n",
        "        #     self._the_weight.append(random.random())\n",
        "        #\n",
        "        # self._the_weight = np.array(self._the_weight)\n",
        "        # self._the_bias = random.random()\n",
        "        self._x = data_points\n",
        "        self._y_values = y_values\n",
        "        # Implementing Gradient Descent for optimization\n",
        "        for n in range(self.get_num_iterations()):\n",
        "            weight_bias = self.increment_weight_bias(self._x, self._y_values, self.get_weight(), self.get_bias())\n",
        "            # self._the_weight = weight_bias[0]\n",
        "            # self._the_bias = weight_bias[1]\n",
        "            self.update_weight(weight_bias[0])\n",
        "            self.update_bias(weight_bias[1])\n",
        "\n",
        "    def increment_weight_bias(self, data_points, y_values, weights, bias) -> Tuple:\n",
        "        # Changing labels\n",
        "        y_values = np.asarray(y_values)\n",
        "        for i in range(len(y_values)):\n",
        "            if y_values[i] <= 0:\n",
        "                y_values[i] = -1\n",
        "        y_label = y_values\n",
        "\n",
        "        # Gradients (dw, db)\n",
        "        for i in range(len(data_points)):\n",
        "            constraint = y_label[i] * (np.dot(data_points[i], weights) - bias) >= 1\n",
        "            if constraint is True:\n",
        "                dw = 2 * self.get_lambda_value() * weights\n",
        "                db = 0\n",
        "            else:\n",
        "                dw = 2 * self.get_lambda_value() * weights - np.dot(data_points[i], y_label[i])\n",
        "                db = y_label[i]\n",
        "\n",
        "        weights = weights - self.return_learning_rate() * dw\n",
        "        bias = bias - self.return_learning_rate() * db\n",
        "        weight_bias = (weights, bias)\n",
        "        return weight_bias\n",
        "\n",
        "    def estimate(self, data_points):\n",
        "        results = np.dot(data_points, self.get_weight()) - self.get_bias()\n",
        "        predicted_labels = np.sign(results)\n",
        "        for i in range(len(predicted_labels)):\n",
        "            if predicted_labels[i] <= -1:\n",
        "                predicted_labels[i] = 0\n",
        "        return predicted_labels\n",
        "\n",
        "    def one_estimate(self, data_point):\n",
        "        result = np.dot(data_point, self.get_weight()) - self.get_bias()\n",
        "        predicted_label = np.sign(result)\n",
        "        if predicted_label <= -1:\n",
        "            predicted_label = 0\n",
        "        return predicted_label"
      ],
      "metadata": {
        "id": "43FtbBTIsxHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Standard scaler and splitting of the data.\n",
        "- standard_scaler uses means and std deviations to standardize the features.\n",
        "- split splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "XUE7o6X1s7nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(features):\n",
        "    # Computation of mean and standard deviation\n",
        "    means = features.mean(axis=0)\n",
        "    std_deviations = features.std(axis=0)\n",
        "    # Standardizing the features\n",
        "    standardized_features = (features - means) / std_deviations\n",
        "    standardized_features = np.array(standardized_features)\n",
        "    return standardized_features\n",
        "\n",
        "def _random_indexes(array, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > array:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(array))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, array, 1)\n",
        "    random_indexes.append(random_index)\n",
        "\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, array, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, array, 1)\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "    return random_indexes\n",
        "\n",
        "def split(features, targets, test_size, random_state=42):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "Ou5O8WJYtKQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "XCvvavd5tbOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_positive += 1\n",
        "\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "            else:\n",
        "                false_negative += 1\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "VTcJ5NnSts5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Main function."
      ],
      "metadata": {
        "id": "syznJg8FuAPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p08uZNB-npJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68dc0cb-6525-4fee-95f6-83bf9ad51d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score on the validation data:  0.25\n",
            "Precision score on the validation data:  1.0\n",
            "Recall score on the validation data:  0.25\n",
            "F1 score on the validation data:  0.4\n",
            "The confusion matrix on the validation data:  [[3, 0], [9, 0]]\n",
            "1.0   0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    mfcc, status = extract_mfcc()\n",
        "    features = standard_scaler(mfcc)\n",
        "    x_train, x_valid, y_train, y_valid = split(mfcc, status, test_size=0.20)\n",
        "    #y_test = list(y_test)\n",
        "    the_weight = []\n",
        "    for c in range(13):\n",
        "        the_weight.append(random.random())\n",
        "    the_weight = np.array(the_weight)\n",
        "    the_bias = random.random()\n",
        "    svm = SupportVectorMachine(learning_rate=0.1, num_iterations=10000, lambda_value=0.1, weight=the_weight, bias=the_bias)\n",
        "    svm.fitting(x_train, y_train)\n",
        "    predictions = svm.estimate(x_valid)\n",
        "\n",
        "    # Metrics on the validation data\n",
        "    accuracy = accuracy_score(y_valid, predictions)\n",
        "    precision = precision_score(y_valid, predictions)\n",
        "    recall = recall_score(y_valid, predictions)\n",
        "    f1 = f1_score(y_valid, predictions)\n",
        "    confusion_mat = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "    print('Accuracy score on the validation data: ', accuracy)\n",
        "    print('Precision score on the validation data: ', precision)\n",
        "    print('Recall score on the validation data: ', recall)\n",
        "    print('F1 score on the validation data: ', f1)\n",
        "    print('The confusion matrix on the validation data: ', confusion_mat)\n",
        "    mfcc_test = x_valid[7]\n",
        "    pred = svm.one_estimate(mfcc_test)\n",
        "    print(pred, ' ', y_valid[7])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95OaiBwgjjNX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}