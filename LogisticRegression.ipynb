{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parkinson's Disease detection with zero crossing rate audio features using Logistic Regression coded without libraries."
      ],
      "metadata": {
        "id": "2RSTiaBDIJ_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "5sOni2KlYoXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "import random\n",
        "import math\n",
        "import librosa\n",
        "import os"
      ],
      "metadata": {
        "id": "g-wl8Ym6ay5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the audios with Librosa, and preprocess them using Peak Amplitude Normalization.\n",
        "- load_audio will return all the audios with their sampling rate.\n",
        "- to_positive will turn all values to postive numbers (audio is 1D).\n",
        "- get_max will return the maximum value in each audio array.\n",
        "- peak_amplitude_normalize will preprocess audios with this formula 10 ** (peak / 20) / maximum_value.\n",
        "- hamming_window uses this formula 0.54 - 0.46 * math.cos((2 * math.pi * n) / frame_length - 1) to every frame of the audio and returns windowed audios.\n",
        "- frame_audio_signal uses hamming_window on frames of audios and returns framed audios."
      ],
      "metadata": {
        "id": "2E5EQBNea4KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path: str) -> Tuple:\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=44100)\n",
        "    return audio, sampling_rate\n",
        "\n",
        "\n",
        "def to_positive(n_array):  # Turn all values to positive values\n",
        "    for i in range(len(n_array)):\n",
        "        if n_array[i] < 0:\n",
        "            n_array[i] = -1 * n_array[i]\n",
        "\n",
        "    return n_array\n",
        "\n",
        "\n",
        "def get_max(n_array):  # Get the maximum value\n",
        "    max_value = n_array[0]\n",
        "    for i in range(1, len(n_array), 1):\n",
        "        if n_array[i] > max_value:\n",
        "            max_value = n_array[i]\n",
        "\n",
        "    return max_value\n",
        "\n",
        "\n",
        "def peak_amplitude_normalize(audio_data, peak=-3.0):  # Calculate a scaling factor based on the specific peak value\n",
        "    n_array = to_positive(audio_data)               # (-3 dB) and multiply the entire audio signal by the scaling factor\n",
        "    maximum_value = get_max(n_array)\n",
        "    scaling = 10 ** (peak / 20) / maximum_value\n",
        "    normalized_audio = audio_data * scaling\n",
        "    return normalized_audio\n",
        "\n",
        "\n",
        "def hamming_window(frame_length):\n",
        "    window = np.zeros(frame_length)\n",
        "    n_negative_one = frame_length - 1\n",
        "    for n in range(frame_length):\n",
        "        window[n] = 0.54 - 0.46 * math.cos((2 * math.pi * n) / n_negative_one)\n",
        "    return window\n",
        "\n",
        "\n",
        "# Function to frame the signal using a Hamming window\n",
        "def frame_audio_signal(audio_data, frame_length):\n",
        "    length_audio = len(audio_data)\n",
        "    number_frames = length_audio // frame_length\n",
        "    framed_audio = np.zeros((frame_length, number_frames))\n",
        "    for i in range(number_frames):\n",
        "        start = i * frame_length\n",
        "        stop = start + frame_length\n",
        "        samples = audio_data[start:stop] * hamming_window(frame_length)\n",
        "        framed_audio[:, i] = samples\n",
        "\n",
        "    return framed_audio"
      ],
      "metadata": {
        "id": "3jfVYpujdgjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract Zero Cross rating features.\n",
        "- zero_crossing uses Numpy to count how many times the audio crosses zero.\n",
        "- zero_crossing_features uses zero_crossing to extract features for all audios."
      ],
      "metadata": {
        "id": "OE3xE_p5dqT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_crossing(frames):\n",
        "    zcr = np.mean(np.abs(np.diff(np.sign(frames))), axis=1)\n",
        "    return zcr\n",
        "\n",
        "\n",
        "def zero_crossing_features() -> Tuple:\n",
        "    directory = '/content/drive/MyDrive/HYP TRAIN DATA/'\n",
        "    zcr_features = []\n",
        "    status = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            if \"P\" in file_name:\n",
        "                status.append(1)\n",
        "\n",
        "            if \"C\" in file_name:\n",
        "                status.append(0)\n",
        "\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            audio, sampling_rate = load_audio(file_path)\n",
        "            frame_time = 0.02  # Duration of each frame in seconds\n",
        "            frame_size = int(frame_time * sampling_rate)  # Number of samples in each frame\n",
        "            preprocessed_audio = peak_amplitude_normalize(audio)\n",
        "            frames = frame_audio_signal(preprocessed_audio, frame_size)\n",
        "            zcr = zero_crossing(frames)\n",
        "            zcr_features.append(np.array([zcr.mean()]))\n",
        "\n",
        "    zcr_features, status = np.array(zcr_features), np.array(status)\n",
        "    return zcr_features, status"
      ],
      "metadata": {
        "id": "A1ZjKRCaeelJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Logistic Regression class with data members learning rate, number of iterations, weights and bias.\n",
        "- Logistic Regression uses Sigmoid function to turn every value to 1 or 0.\n",
        "- The weights and bias are derived using Gradient Descent."
      ],
      "metadata": {
        "id": "V9AYPO-1elbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    # Constructor for setting learning rate, number of iterations, weight, and bias\n",
        "    def __init__(self, learning_rate, num_iterations, weight, bias):\n",
        "        self._learning_rate = learning_rate\n",
        "        self._num_iterations = num_iterations\n",
        "        self._weight = weight\n",
        "        self._bias = bias\n",
        "\n",
        "    # Getter - get the learning rate\n",
        "    def get_learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    # Getter - get the number of iterations\n",
        "    def get_num_iterations(self):\n",
        "        return self._num_iterations\n",
        "\n",
        "    def update_weight(self, weight):\n",
        "        self._weight = weight\n",
        "\n",
        "    def update_bias(self, bias):\n",
        "        self._bias = bias\n",
        "\n",
        "    def return_weight(self):\n",
        "        return self._weight\n",
        "\n",
        "    def return_bias(self):\n",
        "        return self._bias\n",
        "\n",
        "    # For fitting the dataset\n",
        "    def fitting(self, data_points, y_values):\n",
        "        self._num_rows, self._num_cols = self.shape(data_points)  # Tuple - shape returns num of rows and cols\n",
        "\n",
        "        # Setting the weight and bias values to zeros\n",
        "        # self._we8ght = np.zeros(self._num_cols)\n",
        "        self._we8ght = []\n",
        "\n",
        "        for c in range(self._num_cols):\n",
        "            self._we8ght.append(random.random())\n",
        "\n",
        "        self._we8ght = np.array(self._we8ght)\n",
        "        self._byas = random.random()\n",
        "        self._x_data_points = data_points\n",
        "        self._y_values = y_values\n",
        "\n",
        "        # Gradient Descent implementation\n",
        "        for n in range(self.get_num_iterations()):\n",
        "            weight_bias = self.increment_weight_bias(self._x_data_points, self._we8ght, self._byas, self._y_values)\n",
        "            self._we8ght = weight_bias[0]\n",
        "            self._byas = weight_bias[1]\n",
        "            self.update_weight(self._we8ght)\n",
        "            self.update_bias(self._byas)\n",
        "\n",
        "    def zee_formula(self, data_points, weights, bias):\n",
        "        self._data_points = data_points\n",
        "        self._weights = weights\n",
        "        self._bias_ = bias\n",
        "        return np.dot(self._data_points, self._weights) + self._bias_\n",
        "\n",
        "    def shape(self, x) -> Tuple:\n",
        "        row, col = len(x), len(x[0])\n",
        "        self._row, self._col = row, col\n",
        "        self._row_col = (self._row, self._col)\n",
        "        return self._row_col\n",
        "\n",
        "    def exponential(self, array):\n",
        "        exp = []\n",
        "        for v in array:\n",
        "            value = math.exp(v)\n",
        "            exp.append(value)\n",
        "\n",
        "        exp = np.array(exp)\n",
        "        return exp\n",
        "\n",
        "    def summation(self, array):\n",
        "        sum = 0\n",
        "        for v in array:\n",
        "            sum += v\n",
        "\n",
        "        return sum\n",
        "\n",
        "    def increment_weight_bias(self, data_points, weights, bias, y_values) -> Tuple:\n",
        "        # Sigmoid formula\n",
        "        z = self.zee_formula(data_points, weights, bias)\n",
        "        y_sigmoid = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        rows_cols = self.shape(data_points)\n",
        "        self._xx = data_points\n",
        "        self._yy = y_values\n",
        "\n",
        "        # Gradients\n",
        "        dw = (1 / rows_cols[0]) * np.dot(self._xx.T, (y_sigmoid - self._yy))\n",
        "        db = (1 / rows_cols[1]) * self.summation(y_sigmoid - self._yy)\n",
        "\n",
        "        # Incrementing the weights and bias\n",
        "        self._we = weights\n",
        "        self._bi = bias\n",
        "        self._the_weight = self._we - self.get_learning_rate() * dw\n",
        "        self._the_bias = self._bi - self.get_learning_rate() * db\n",
        "        weight_bias = (self._the_weight, self._the_bias)\n",
        "\n",
        "        return weight_bias\n",
        "\n",
        "    def predict(self, data_points):\n",
        "        self._X = data_points\n",
        "        zee = self.zee_formula(self._X, self.return_weight(), self.return_bias())\n",
        "        y_predictions = 1 / (1 + self.exponential(-zee))\n",
        "\n",
        "        for i in range(len(y_predictions)):\n",
        "            if y_predictions[i] > 0.5:\n",
        "                y_predictions[i] = 1\n",
        "\n",
        "            else:\n",
        "                y_predictions[i] = 0\n",
        "\n",
        "        return y_predictions\n",
        "\n",
        "    def single_predict(self, data_point):\n",
        "        self._X = data_point\n",
        "        zee = self.zee_formula(self._X, self.return_weight(), self.return_bias())\n",
        "        y_prediction = 1 / (1 + np.exp(-zee))\n",
        "\n",
        "        if y_prediction > 0.5:\n",
        "            y_prediction = 1\n",
        "\n",
        "        else:\n",
        "            y_prediction = 0\n",
        "\n",
        "        return y_prediction"
      ],
      "metadata": {
        "id": "tPg6l16HgD7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Standard scaler and splitting of the data.\n",
        "- standard_scaler uses means and std deviations to standardize the features.\n",
        "- split splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "WmLR64uTgXCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(features):\n",
        "    # Computation of mean and standard deviation\n",
        "    means = features.mean(axis=0)\n",
        "    std_deviations = features.std(axis=0)\n",
        "\n",
        "    # Standardizing the features\n",
        "    standardized_features = (features - means) / std_deviations\n",
        "    standardized_features = np.array(standardized_features)\n",
        "\n",
        "    return standardized_features\n",
        "\n",
        "\n",
        "def _random_indexes(number, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > number:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(number))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, number, 1)\n",
        "    random_indexes.append(random_index)\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, number, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, number, 1)\n",
        "\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "\n",
        "    return random_indexes\n",
        "\n",
        "\n",
        "def split(features, targets, test_size, random_state=42):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "kk3ft8dzhBlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "kWjGed55hKnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "\n",
        "            else:\n",
        "                false_positive += 1\n",
        "\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "\n",
        "            else:\n",
        "                false_negative += 1\n",
        "\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "Cs8YNQeAiZbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Main function.\n"
      ],
      "metadata": {
        "id": "O2Iobteeij2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7usfDiZ3P0g",
        "outputId": "8fcf11e3-4276-40f1-98af-7be84aa1ec4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on the validation data.\n",
            "Accuracy score on the validation data:  0.75\n",
            "Precision score on the validation data:  0.0\n",
            "Recall score on the validation data:  0.0\n",
            "F1 score on the validation data:  0.0\n",
            "Confusion matrix on the validation data:  [[0, 3], [0, 9]]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    zcr_feats, y_labels = zero_crossing_features()\n",
        "    features = standard_scaler(zcr_feats)\n",
        "    x_train, x_valid, y_train, y_valid = split(features, y_labels, test_size=0.2)\n",
        "\n",
        "    model = LogisticRegression(learning_rate=0.01, num_iterations=10000, weight=0, bias=0)\n",
        "    model.fitting(x_train, y_train)\n",
        "    predictions = model.predict(x_valid)\n",
        "\n",
        "    # Metrics on the validation data\n",
        "    accuracy = accuracy_score(y_valid, predictions)\n",
        "    precision = precision_score(y_valid, predictions)\n",
        "    recall = recall_score(y_valid, predictions)\n",
        "    f1 = f1_score(y_valid, predictions)\n",
        "    confusion_mat = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "    print(\"Evaluation on the validation data.\")\n",
        "    print(\"Accuracy score on the validation data: \", accuracy)\n",
        "    print(\"Precision score on the validation data: \", precision)\n",
        "    print(\"Recall score on the validation data: \", recall)\n",
        "    print(\"F1 score on the validation data: \", f1)\n",
        "    print(\"Confusion matrix on the validation data: \", confusion_mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrPFOXeF7xWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}