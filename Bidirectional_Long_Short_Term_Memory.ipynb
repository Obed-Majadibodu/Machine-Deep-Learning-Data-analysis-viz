{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disaster related tweet text data classification with Bidirectional Long-Short Term Memory (B-LSTM)"
      ],
      "metadata": {
        "id": "ks0QXP5LCszu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "p0R3nBF3-Z1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "_ph0oWP2-hOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split data function.\n",
        "- _random_indexes find indexes of the testing set.\n",
        "- split_data splits the data into training and testing data."
      ],
      "metadata": {
        "id": "KuKUYDHh-lJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _random_indexes(number, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > number:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(number))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, number, 1)\n",
        "    random_indexes.append(random_index)\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, number, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, number, 1)\n",
        "\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "\n",
        "    return random_indexes\n",
        "\n",
        "\n",
        "def split_data(features, targets, test_size, random_state=4):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "ay2hRL7--2SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load the datasets.\n",
        "- Load the training and testing data.\n",
        "- Tokenize the sentences of each tweet.\n",
        "- Turn tokens into text sequences.\n"
      ],
      "metadata": {
        "id": "MMnEfUy7_B2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/NLP TRAIN AND TEST/nlp_tweet_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/NLP TRAIN AND TEST/nlp_tweet_test.csv\")\n",
        "\n",
        "X_train_sentences, X_valid_sentences, y_train_labels, y_valid_labels = split_data(train_data[\"text\"], train_data[\"target\"], test_size=0.2, random_state=42)\n",
        "X_test_sentences = test_data[\"text\"]\n",
        "y_test_labels = test_data[\"target\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train_sentences)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=100, padding='post', truncating='post')\n",
        "X_valid_sequences = tokenizer.texts_to_sequences(X_valid_sentences)\n",
        "X_valid_padded = pad_sequences(X_valid_sequences, maxlen=100, padding='post', truncating='post')\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test_sentences)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "X_train_padded = np.array(X_train_padded)\n",
        "X_valid_padded = np.array(X_valid_padded)\n",
        "y_train_labels = np.array(y_train_labels)\n",
        "y_valid_labels = np.array(y_valid_labels)\n",
        "X_test_padded = np.array(X_test_padded)\n",
        "y_test_labels = np.array(y_test_labels)"
      ],
      "metadata": {
        "id": "p3klG2LcA2Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Build a B-LSTM model.\n",
        "- Create a Sequential model.\n",
        "- Add word embedding layer.\n",
        "- Add a bidirectionl layer with LSTM layer.\n",
        "- Finally add two densely connected layers with ReLU, and sigmoid activation functions respectively.\n",
        "- Compile the model with 'binary_crossentropy', 'adam', and 'accuracy'.\n",
        "- Fit the model to the training data.\n",
        "- Evaluate the model and make predictions."
      ],
      "metadata": {
        "id": "0hVr8FVRA9m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(100, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(X_train_padded, y_train_labels, epochs=30, validation_data=(X_valid_padded, y_valid_labels), verbose=1)\n",
        "loss, accuracy = model.evaluate(X_valid_padded, y_valid_labels)\n",
        "print(\"Loss: \", loss, \", accuracy: \", accuracy)\n",
        "\n",
        "temp_valid_preds = model.predict(X_valid_padded)\n",
        "temp_test_preds = model.predict(X_test_padded)\n",
        "valid_preds, test_preds = [], []\n",
        "threshold = 0.5\n",
        "\n",
        "for temp_valid_pred in temp_valid_preds:\n",
        "  if temp_valid_pred >= threshold:\n",
        "    valid_preds.append(1)\n",
        "  else:\n",
        "    valid_preds.append(0)\n",
        "\n",
        "for temp_test_pred in temp_test_preds:\n",
        "  if temp_test_pred >= threshold:\n",
        "    test_preds.append(1)\n",
        "  else:\n",
        "    test_preds.append(0)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Evaluation on the validation data.\")\n",
        "print(\"Accuracy score on the validation data: \", accuracy_score(y_valid_labels, valid_preds))\n",
        "print(\"Precision score on the validation data: \", precision_score(y_valid_labels, valid_preds))\n",
        "print(\"Recall score on the validation data: \", recall_score(y_valid_labels, valid_preds))\n",
        "print(\"F1 score on the validation data: \", f1_score(y_valid_labels, valid_preds))\n",
        "print(\"Confusion matrix on the validation data: \", confusion_matrix(y_valid_labels, valid_preds))\n",
        "print(\"\\n\")\n",
        "print(\"Evaluation on the testing data.\")\n",
        "print(\"Accuracy score on the testing data: \", accuracy_score(y_test_labels, test_preds))\n",
        "print(\"Precision score on the testing data: \", precision_score(y_test_labels, test_preds))\n",
        "print(\"Recall score on the testing data: \", recall_score(y_test_labels, test_preds))\n",
        "print(\"F1 score on the testing data: \", f1_score(y_test_labels, test_preds))\n",
        "print(\"Confusion matrix on the testing data: \", confusion_matrix(y_test_labels, test_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LlGVP5C_c8L",
        "outputId": "6731e0bb-d940-4ab8-ab3e-8daff4140d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "153/153 [==============================] - 28s 147ms/step - loss: 0.6389 - accuracy: 0.6317 - val_loss: 0.5605 - val_accuracy: 0.7211\n",
            "Epoch 2/30\n",
            "153/153 [==============================] - 21s 135ms/step - loss: 0.5692 - accuracy: 0.7058 - val_loss: 0.5496 - val_accuracy: 0.7186\n",
            "Epoch 3/30\n",
            "153/153 [==============================] - 22s 145ms/step - loss: 0.5634 - accuracy: 0.7089 - val_loss: 0.5533 - val_accuracy: 0.7014\n",
            "Epoch 4/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.5593 - accuracy: 0.7142 - val_loss: 0.5460 - val_accuracy: 0.7129\n",
            "Epoch 5/30\n",
            "153/153 [==============================] - 21s 139ms/step - loss: 0.5504 - accuracy: 0.7200 - val_loss: 0.5605 - val_accuracy: 0.6998\n",
            "Epoch 6/30\n",
            "153/153 [==============================] - 19s 127ms/step - loss: 0.5492 - accuracy: 0.7241 - val_loss: 0.5541 - val_accuracy: 0.7047\n",
            "Epoch 7/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.5447 - accuracy: 0.7214 - val_loss: 0.5528 - val_accuracy: 0.7088\n",
            "Epoch 8/30\n",
            "153/153 [==============================] - 21s 137ms/step - loss: 0.5412 - accuracy: 0.7245 - val_loss: 0.5530 - val_accuracy: 0.7088\n",
            "Epoch 9/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.5410 - accuracy: 0.7307 - val_loss: 0.5847 - val_accuracy: 0.7162\n",
            "Epoch 10/30\n",
            "153/153 [==============================] - 22s 146ms/step - loss: 0.5410 - accuracy: 0.7255 - val_loss: 0.5735 - val_accuracy: 0.7121\n",
            "Epoch 11/30\n",
            "153/153 [==============================] - 20s 129ms/step - loss: 0.5330 - accuracy: 0.7317 - val_loss: 0.5551 - val_accuracy: 0.7219\n",
            "Epoch 12/30\n",
            "153/153 [==============================] - 22s 145ms/step - loss: 0.5281 - accuracy: 0.7333 - val_loss: 0.5627 - val_accuracy: 0.7235\n",
            "Epoch 13/30\n",
            "153/153 [==============================] - 20s 129ms/step - loss: 0.5235 - accuracy: 0.7399 - val_loss: 0.5675 - val_accuracy: 0.7178\n",
            "Epoch 14/30\n",
            "153/153 [==============================] - 20s 130ms/step - loss: 0.5151 - accuracy: 0.7382 - val_loss: 0.5713 - val_accuracy: 0.7178\n",
            "Epoch 15/30\n",
            "153/153 [==============================] - 21s 135ms/step - loss: 0.5113 - accuracy: 0.7483 - val_loss: 0.5666 - val_accuracy: 0.7137\n",
            "Epoch 16/30\n",
            "153/153 [==============================] - 19s 126ms/step - loss: 0.5088 - accuracy: 0.7450 - val_loss: 0.5719 - val_accuracy: 0.7252\n",
            "Epoch 17/30\n",
            "153/153 [==============================] - 21s 140ms/step - loss: 0.5040 - accuracy: 0.7471 - val_loss: 0.5773 - val_accuracy: 0.7121\n",
            "Epoch 18/30\n",
            "153/153 [==============================] - 19s 126ms/step - loss: 0.5034 - accuracy: 0.7434 - val_loss: 0.5782 - val_accuracy: 0.7153\n",
            "Epoch 19/30\n",
            "153/153 [==============================] - 21s 139ms/step - loss: 0.4930 - accuracy: 0.7522 - val_loss: 0.5846 - val_accuracy: 0.7014\n",
            "Epoch 20/30\n",
            "153/153 [==============================] - 20s 130ms/step - loss: 0.4908 - accuracy: 0.7520 - val_loss: 0.5918 - val_accuracy: 0.7170\n",
            "Epoch 21/30\n",
            "153/153 [==============================] - 20s 129ms/step - loss: 0.4821 - accuracy: 0.7520 - val_loss: 0.6046 - val_accuracy: 0.7055\n",
            "Epoch 22/30\n",
            "153/153 [==============================] - 21s 138ms/step - loss: 0.4767 - accuracy: 0.7606 - val_loss: 0.6140 - val_accuracy: 0.7186\n",
            "Epoch 23/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.4763 - accuracy: 0.7621 - val_loss: 0.6058 - val_accuracy: 0.7203\n",
            "Epoch 24/30\n",
            "153/153 [==============================] - 21s 140ms/step - loss: 0.4681 - accuracy: 0.7590 - val_loss: 0.6114 - val_accuracy: 0.7194\n",
            "Epoch 25/30\n",
            "153/153 [==============================] - 19s 126ms/step - loss: 0.4605 - accuracy: 0.7617 - val_loss: 0.6180 - val_accuracy: 0.7112\n",
            "Epoch 26/30\n",
            "153/153 [==============================] - 21s 137ms/step - loss: 0.4569 - accuracy: 0.7651 - val_loss: 0.6423 - val_accuracy: 0.7080\n",
            "Epoch 27/30\n",
            "153/153 [==============================] - 19s 127ms/step - loss: 0.4462 - accuracy: 0.7711 - val_loss: 0.6395 - val_accuracy: 0.7030\n",
            "Epoch 28/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.4365 - accuracy: 0.7785 - val_loss: 0.6964 - val_accuracy: 0.6973\n",
            "Epoch 29/30\n",
            "153/153 [==============================] - 21s 140ms/step - loss: 0.4390 - accuracy: 0.7664 - val_loss: 0.6509 - val_accuracy: 0.6874\n",
            "Epoch 30/30\n",
            "153/153 [==============================] - 20s 128ms/step - loss: 0.4244 - accuracy: 0.7797 - val_loss: 0.6804 - val_accuracy: 0.6891\n",
            "39/39 [==============================] - 1s 27ms/step - loss: 0.6804 - accuracy: 0.6891\n",
            "Loss:  0.6803502440452576 , accuracy:  0.6890894174575806\n",
            "39/39 [==============================] - 2s 42ms/step\n",
            "48/48 [==============================] - 2s 39ms/step\n",
            "Evaluation on the validation data.\n",
            "Accuracy score on the validation data:  0.6890894175553732\n",
            "Precision score on the validation data:  0.6380368098159509\n",
            "Recall score on the validation data:  0.6070038910505836\n",
            "F1 score on the validation data:  0.6221335992023928\n",
            "Confusion matrix on the validation data:  [[528 177]\n",
            " [202 312]]\n",
            "\n",
            "\n",
            "Evaluation on the testing data.\n",
            "Accuracy score on the testing data:  0.7025607353906763\n",
            "Precision score on the testing data:  0.6555555555555556\n",
            "Recall score on the testing data:  0.6363636363636364\n",
            "F1 score on the testing data:  0.6458170445660673\n",
            "Confusion matrix on the testing data:  [[657 217]\n",
            " [236 413]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hL-K_GoooQ2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}