{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disaster related tweet text data classification with Logistic Regression."
      ],
      "metadata": {
        "id": "4KQxwUzjB9Nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "tBBH3U5IZz9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "s0PLWaFDZ1h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split data function.\n",
        "- _random_indexes find indexes of the testing set.\n",
        "- split_data splits the data into training and testing data."
      ],
      "metadata": {
        "id": "Osvi7YpkZ6la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _random_indexes(number, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > number:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(number))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, number, 1)\n",
        "    random_indexes.append(random_index)\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, number, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, number, 1)\n",
        "\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "\n",
        "    return random_indexes\n",
        "\n",
        "\n",
        "def split_data(features, targets, test_size, random_state=4):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "Or7xGQD9aBfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "44bTeU9NaLtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "\n",
        "            else:\n",
        "                false_positive += 1\n",
        "\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "\n",
        "            else:\n",
        "                false_negative += 1\n",
        "\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "LQwlZkeTaXCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Main function.\n",
        "- Split data into train and validation set.\n",
        "- Extracts Term-Frequency Inverse Document Frequency (TF-IDF).\n",
        "- Create a Logistic Regression model and fit the train data.\n",
        "- Evaluate the model."
      ],
      "metadata": {
        "id": "IW9uzzqbapM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT1KAjgiH-iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d619e64-d302-4753-fe17-10ed2086cbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on the validation data.\n",
            "Accuracy score on the validation data: 0.8129614438063987\n",
            "Precision score on the validation data: 0.6984435797665369\n",
            "Recall score on the validation data: 0.8310185185185185\n",
            "F1 score on the validation data: 0.7589852008456659\n",
            "Confusion matrix on the validation data: [[359, 155], [73, 632]]\n",
            "\n",
            " \n",
            "\n",
            "Evaluation on the testing data.\n",
            "Accuracy score on the testing data: 0.7931713722915299\n",
            "Precision score on the testing data: 0.6825885978428351\n",
            "Recall score on the testing data: 0.802536231884058\n",
            "F1 score on the testing data: 0.7377185678601166\n",
            "Confusion matrix on the testing data: [[443, 206], [109, 765]]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "   train_data = pd.read_csv(\"/content/drive/MyDrive/NLP TRAIN AND TEST/nlp_tweet_train.csv\")\n",
        "   test_data = pd.read_csv(\"/content/drive/MyDrive/NLP TRAIN AND TEST/nlp_tweet_test.csv\")\n",
        "\n",
        "   X_train, X_valid, y_train, y_valid = split_data(train_data[\"text\"], train_data[\"target\"], test_size=0.2, random_state=42)\n",
        "\n",
        "   vectorizer = TfidfVectorizer(max_features=5000)\n",
        "   vectorized_X_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "   logistic_reg_classifier = LogisticRegression()\n",
        "   logistic_reg_classifier.fit(vectorized_X_train, y_train)\n",
        "   vectorized_X_valid = vectorizer.transform(X_valid)\n",
        "\n",
        "   X_valid_preds = logistic_reg_classifier.predict(vectorized_X_valid)\n",
        "\n",
        "   valid_accuracy = accuracy_score(y_valid, X_valid_preds)\n",
        "   valid_precision = precision_score(y_valid, X_valid_preds)\n",
        "   valid_recall = recall_score(y_valid, X_valid_preds)\n",
        "   valid_f1 = f1_score(y_valid, X_valid_preds)\n",
        "   valid_confusion_mat = confusion_matrix(y_valid, X_valid_preds)\n",
        "   print(\"Evaluation on the validation data.\")\n",
        "   print(\"Accuracy score on the validation data:\", valid_accuracy)\n",
        "   print(\"Precision score on the validation data:\", valid_precision)\n",
        "   print(\"Recall score on the validation data:\", valid_recall)\n",
        "   print(\"F1 score on the validation data:\", valid_f1)\n",
        "   print(\"Confusion matrix on the validation data:\", valid_confusion_mat)\n",
        "\n",
        "   vectorized_X_test = vectorizer.transform(test_data[\"text\"])\n",
        "   y_test = test_data[\"target\"]\n",
        "   X_test_preds = logistic_reg_classifier.predict(vectorized_X_test)\n",
        "\n",
        "   print(\"\\n \\n\")\n",
        "   test_accuracy = accuracy_score(y_test, X_test_preds)\n",
        "   test_precision = precision_score(y_test, X_test_preds)\n",
        "   test_recall = recall_score(y_test, X_test_preds)\n",
        "   test_f1 = f1_score(y_test, X_test_preds)\n",
        "   test_confusion_mat = confusion_matrix(y_test, X_test_preds)\n",
        "   print(\"Evaluation on the testing data.\")\n",
        "   print(\"Accuracy score on the testing data:\", test_accuracy)\n",
        "   print(\"Precision score on the testing data:\", test_precision)\n",
        "   print(\"Recall score on the testing data:\", test_recall)\n",
        "   print(\"F1 score on the testing data:\", test_f1)\n",
        "   print(\"Confusion matrix on the testing data:\", test_confusion_mat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T1YdFsNvm90g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}