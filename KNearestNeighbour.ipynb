{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parkinson's Disease detection with energy audio features using K-Nearest Neighbors coded without libraries."
      ],
      "metadata": {
        "id": "p4NsDUg8Izgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "Bz2TUYRCkNlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "import os\n",
        "import random\n",
        "import librosa\n",
        "import math"
      ],
      "metadata": {
        "id": "ai5aSqhnkdrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the audios with Librosa, and preprocess them using Peak Amplitude Normalization.\n",
        "- load_audio will return all the audios with their sampling rate.\n",
        "- to_positive will turn all values to postive numbers (audio is 1D).\n",
        "- get_max will return the maximum value in each audio array.\n",
        "- peak_amplitude_normalize will preprocess audios with this formula 10 ** (peak / 20) / maximum_value.\n",
        "- hamming_window uses this formula 0.54 - 0.46 * math.cos((2 * math.pi * n) / frame_length - 1) to every frame of the audio and returns windowed audios.\n",
        "- frame_audio_signal uses hamming_window on frames of audios and returns framed audios."
      ],
      "metadata": {
        "id": "Cm4khlR5kg6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path):  # This function loads the audios\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=44100)\n",
        "    return audio, sampling_rate\n",
        "\n",
        "def to_positive(n_array):  # Turn all values to positive values\n",
        "    for i in range(len(n_array)):\n",
        "        if n_array[i] < 0:\n",
        "            n_array[i] = -1 * n_array[i]\n",
        "    return n_array\n",
        "\n",
        "def get_max(n_array):  # Get the maximum value\n",
        "    max_value = n_array[0]\n",
        "    for i in range(1, len(n_array), 1):\n",
        "        if n_array[i] > max_value:\n",
        "            max_value = n_array[i]\n",
        "    return max_value\n",
        "\n",
        "def peak_amplitude_normalize(audio_data, peak=-3.0):  # Calculate a scaling factor based on the specific peak value\n",
        "    n_array = to_positive(audio_data)               # (-3 dB) and multiply the entire audio signal by the scaling factor\n",
        "    maximum_value = get_max(n_array)\n",
        "    scaling = 10 ** (peak / 20) / maximum_value\n",
        "    normalized_audio = audio_data * scaling\n",
        "    return normalized_audio\n",
        "\n",
        "def hamming_window(frame_length):\n",
        "    window = np.zeros(frame_length)\n",
        "    n_negative_one = frame_length - 1\n",
        "    for n in range(frame_length):\n",
        "        window[n] = 0.54 - 0.46 * math.cos((2 * math.pi * n) / n_negative_one)\n",
        "    return window\n",
        "\n",
        "# Function to frame the signal using a Hamming window\n",
        "def frame_audio_signal(audio_data, frame_length):\n",
        "    length_audio = len(audio_data)\n",
        "    number_frames = length_audio // frame_length\n",
        "    framed_audio = np.zeros((frame_length, number_frames))\n",
        "    for i in range(number_frames):\n",
        "        start = i * frame_length\n",
        "        stop = start + frame_length\n",
        "        samples = audio_data[start:stop] * hamming_window(frame_length)\n",
        "        framed_audio[:, i] = samples\n",
        "    return framed_audio"
      ],
      "metadata": {
        "id": "RcxcHyNmljPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract energy audio features.\n",
        "- extract_energy_frames takes sum of squared frames in each audio.\n",
        "- energy_features uses extract_energy to extract features for all audios."
      ],
      "metadata": {
        "id": "FM08herblq-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract energy using hamming window and frame the signal\n",
        "def extract_energy_frames(audio_data, frame_length):\n",
        "    framed_audio = frame_audio_signal(audio_data, frame_length)\n",
        "    energy = np.sum(framed_audio ** 2, axis=0)\n",
        "    return energy\n",
        "\n",
        "def energy_features() -> Tuple:  # This gets the audios from the dataset, gets their status 0 or 1, load them, clean\n",
        "    feats = []                   # them, and calculate their energy frames as features\n",
        "    status = []\n",
        "    directory = '/content/drive/MyDrive/HYP TRAIN DATA/'\n",
        "    # Getting each audio path from the dataset\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            if \"P\" in file_name:\n",
        "                status.append(1)\n",
        "            if \"C\" in file_name:\n",
        "                status.append(0)\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            audio_data, frame_rate = load_audio(file_path)\n",
        "            frame_time = 5 # Duration of each frame in seconds\n",
        "            frame_size = int(frame_time * frame_rate)  # Number of samples in each frame\n",
        "            preprocessed_audio = peak_amplitude_normalize(audio_data)\n",
        "            energy_feat = extract_energy_frames(audio_data=preprocessed_audio, frame_length=frame_size)\n",
        "            feats.append(energy_feat)\n",
        "    feats = np.array(feats)\n",
        "    status = np.array(status)\n",
        "    return feats, status"
      ],
      "metadata": {
        "id": "YCpEGJOUmihA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. K-Nearest Neighbor class with data members k_nearest, x_train, and y_train.\n",
        "- K-Nearest Neighbor computes the distance between all testing points in the testing data to all data points in the training set and then hold majority vote according to the value of k_nearest."
      ],
      "metadata": {
        "id": "tlIOznfBmpfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KayNearestNeighbour:\n",
        "    def __init__(self, k_nearest, x_train_data, y_train_data):  # Constructor\n",
        "        self._k_nearest = k_nearest\n",
        "        self._x_train = x_train_data\n",
        "        self._y_train = y_train_data\n",
        "\n",
        "    def set_k_nearest(self, k_nearest):  # Set number of k near points\n",
        "        self._k_nearest = k_nearest\n",
        "\n",
        "    def get_k_nearest(self) -> int:  # Get the number of k near points\n",
        "        return self._k_nearest\n",
        "\n",
        "    def distance(self, data_point1, data_point2) -> float:  # Compute the distance between two data points\n",
        "        distance = np.sqrt(np.sum((data_point1 - data_point2) ** 2))  # Euclidean distance\n",
        "        # distance = np.sum(np.abs(data_point1 - data_point2))  # Manhattan distance\n",
        "        # distance = (np.sum(np.abs(data_point1 - data_point2) ** 1)) ** (1 / 1)  # Minkowski distance\n",
        "        return distance\n",
        "\n",
        "    def selection_sort(self, the_list):  # To sort the distances according to their increasing distances\n",
        "        for n in range(len(the_list)):\n",
        "            min_index = n\n",
        "            for m in range(n + 1, len(the_list)):\n",
        "                if the_list[m] < the_list[min_index]:\n",
        "                    min_index = m\n",
        "            the_list[n], the_list[min_index] = the_list[min_index], the_list[n]\n",
        "        return the_list  # Returning the sorted list in ascending order\n",
        "\n",
        "    def data_fitting(self, data_points, y_labels):  # Fit the data\n",
        "        self._x_train = data_points\n",
        "        self._y_train = y_labels\n",
        "\n",
        "    def predict(self, data_points):  # Predict for each given data point a label, using the computed distances\n",
        "        estimates = []\n",
        "        for data_point in data_points:\n",
        "            estimates.append(self.estimate(data_point))\n",
        "        estimates = np.array(estimates)\n",
        "        return estimates\n",
        "\n",
        "    # estimate method first start by computing the distances between the given data point and all data points, then sort\n",
        "    # the computed distances in ascending order, the use the value of k nearest to find the nearest points, the use\n",
        "    # majority vote to find the label\n",
        "    def estimate(self, data_point):\n",
        "        # Computation of the distances\n",
        "        self._y_train = list(self._y_train)\n",
        "        computed_distances = []\n",
        "        for data_point_train in self._x_train:\n",
        "            dist = self.distance(data_point, data_point_train)\n",
        "            computed_distances.append(dist)\n",
        "\n",
        "        computed_distance_labels = []\n",
        "        for n in range(len(computed_distances)):\n",
        "            min_index = n\n",
        "            for m in range(n + 1, len(computed_distances)):\n",
        "                if computed_distances[m] < computed_distances[min_index]:\n",
        "                    min_index = m\n",
        "            computed_distances[n], computed_distances[min_index] = computed_distances[min_index], computed_distances[n]\n",
        "            computed_distance_labels.append(self._y_train[min_index])\n",
        "\n",
        "        closest_distances, closest_distance_labels = [], []\n",
        "        for k in range(self.get_k_nearest()):\n",
        "            closest_distances.append(computed_distances[k])\n",
        "            closest_distance_labels.append(computed_distance_labels[k])\n",
        "\n",
        "        one_status, zero_status = 0, 0\n",
        "        for k in range(len(closest_distance_labels)):\n",
        "            if closest_distance_labels[k] == 1:\n",
        "                one_status += 1\n",
        "            if closest_distance_labels[k] == 0:\n",
        "                zero_status += 1\n",
        "\n",
        "        if one_status > zero_status:\n",
        "            predicted_status = 1\n",
        "        else:\n",
        "            predicted_status = 0\n",
        "        return predicted_status"
      ],
      "metadata": {
        "id": "7AmVOsLjn0yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Standard scaler and splitting of the data.\n",
        "- standard_scaler uses means and std deviations to standardize the features.\n",
        "- split splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "L21iBbxin89-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(features):\n",
        "    # Computation of mean and standard deviation\n",
        "    means = features.mean(axis=0)\n",
        "    std_deviations = features.std(axis=0)\n",
        "    # Standardizing the features\n",
        "    standardized_features = (features - means) / std_deviations\n",
        "    standardized_features = np.array(standardized_features)\n",
        "    return standardized_features\n",
        "\n",
        "def _random_indexes(array, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > array:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(array))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, array, 1)\n",
        "    random_indexes.append(random_index)\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, array, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, array, 1)\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "    return random_indexes\n",
        "\n",
        "def split(features, targets, test_size, random_state=42):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "Ya6osqiXoPZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "jJmBKB6aoWgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_positive += 1\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "            else:\n",
        "                false_negative += 1\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "_ueXQQzVomp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Main function."
      ],
      "metadata": {
        "id": "POX1VZ6fo9ZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA8fii1J7Ryd",
        "outputId": "3f78898e-2955-4568-c30c-40baa32ef6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score on the validation data:  0.5833333333333334\n",
            "Precision score on the validation data:  0.0\n",
            "Recall score on the validation data:  0.0\n",
            "F1 score on the validation data:  0.0\n",
            "The confusion matrix on the validation data:  [[0, 3], [2, 7]]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    energy_feats, status = energy_features()  # get the energy features and the status (0 or 1)\n",
        "    features = standard_scaler(energy_feats)\n",
        "    x_train, x_valid, y_train, y_valid = split(features, status, test_size=0.20)\n",
        "    knn = KayNearestNeighbour(5, x_train, y_train)\n",
        "    knn.data_fitting(x_train, y_train)\n",
        "    predictions = knn.predict(x_valid)\n",
        "\n",
        "    # Metrics on the validation data\n",
        "    accuracy = accuracy_score(y_valid, predictions)\n",
        "    precision = precision_score(y_valid, predictions)\n",
        "    recall = recall_score(y_valid, predictions)\n",
        "    f1 = f1_score(y_valid, predictions)\n",
        "    confusion_mat = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "    print('Accuracy score on the validation data: ', accuracy)\n",
        "    print('Precision score on the validation data: ', precision)\n",
        "    print('Recall score on the validation data: ', recall)\n",
        "    print('F1 score on the validation data: ', f1)\n",
        "    print('The confusion matrix on the validation data: ', confusion_mat)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6YvT232hVVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}