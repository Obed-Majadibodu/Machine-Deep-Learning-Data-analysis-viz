{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parkinson's Disease detection with spectral centroid and spectral rolloff audio features using Decision Tree coded without libraries."
      ],
      "metadata": {
        "id": "-aeJvfKNJyiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the dependencies."
      ],
      "metadata": {
        "id": "_SfoGiygu5i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import librosa\n",
        "from librosa import feature\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "7STRchelu9sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the audios with Librosa, and preprocess them using Peak Amplitude Normalization.\n",
        "- load_audio will return all the audios with their sampling rate.\n",
        "- to_positive will turn all values to postive numbers (audio is 1D).\n",
        "- get_max will return the maximum value in each audio array.\n",
        "- peak_amplitude_normalize will preprocess audios with this formula 10 ** (peak / 20) / maximum_value."
      ],
      "metadata": {
        "id": "MaCrgP36vDso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path) -> Tuple:  # Loads the audio\n",
        "    audio, sampling_rate = librosa.load(file_path, sr=44100)\n",
        "    return audio, sampling_rate\n",
        "\n",
        "def to_positive(n_array):  # Turn all values to positive values\n",
        "    for i in range(len(n_array)):\n",
        "        if n_array[i] < 0:\n",
        "            n_array[i] = -1 * n_array[i]\n",
        "    return n_array\n",
        "\n",
        "def get_max(n_array):  # Get the maximum value\n",
        "    max_value = n_array[0]\n",
        "    for i in range(1, len(n_array), 1):\n",
        "        if n_array[i] > max_value:\n",
        "            max_value = n_array[i]\n",
        "    return max_value\n",
        "\n",
        "def peak_amplitude_normalize(audio_data, peak=-3.0):  # Calculate a scaling factor based on the specific peak value\n",
        "    n_array = to_positive(audio_data)               # (-3 dB) and multiply the entire audio signal by the scaling factor\n",
        "    maximum_value = get_max(n_array)\n",
        "    scaling = 10 ** (peak / 20) / maximum_value\n",
        "    normalized_audio = audio_data * scaling\n",
        "    return normalized_audio"
      ],
      "metadata": {
        "id": "M_tgowJVvSlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract spectral centroid and spectral rolloff audio features.\n",
        "- spectral_centroid_rolloff function extract both spectral centroid and spectral rolloff features using Librosa library for each audio."
      ],
      "metadata": {
        "id": "571McYPgvbTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral_centroid_rolloff() -> Tuple:  # Gets each audio from the dataset, get its status, load the audio, clean\n",
        "    directory = '/content/drive/MyDrive/HYP TRAIN DATA/'  # the audio compute the features (Spectral Centroid & Rolloff)\n",
        "    features = []\n",
        "    status = []\n",
        "    # Getting each audio path from the dataset\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            if \"P\" in file_name:\n",
        "                status.append(1)\n",
        "            if \"C\" in file_name:\n",
        "                status.append(0)\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            audio, sampling_rate = load_audio(file_path)\n",
        "            preprocessed_audio = peak_amplitude_normalize(audio)\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=preprocessed_audio, sr=sampling_rate).mean()\n",
        "            spectral_roll_off = librosa.feature.spectral_rolloff(y=preprocessed_audio, sr=sampling_rate).mean()\n",
        "            feats = np.array([spectral_centroid, spectral_roll_off])\n",
        "            features.append(feats)\n",
        "\n",
        "    features = np.array(features)\n",
        "    status = np.array(status)\n",
        "    return features, status"
      ],
      "metadata": {
        "id": "nxAKdnazwL21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Class Node with data members feature, threshold, and node value and Decision Tree class with data members min_samples_split, max_depth, and n_features.\n",
        "- Every node has a feature to divide with according to the threshold, has a node value, and finally a node has both left and right children (nodes)\n",
        "- Decision Tree builds a tree recursivly starting at the root using the class Node."
      ],
      "metadata": {
        "id": "klegCWtjwUO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A node has feature to divide with, with which threshold, a node has left and right children, and a node has a value\n",
        "# if it is the leaf node\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, node_value=None):  # Constructor\n",
        "        self._feature = feature\n",
        "        self._threshold = threshold\n",
        "        self._node_value = node_value\n",
        "        self._left = None\n",
        "        self._right = None\n",
        "\n",
        "    def set_feature(self, feature):  # To set feature to divide with\n",
        "        self._feature = feature\n",
        "\n",
        "    def set_threshold(self, threshold):  # To set threshold to divide with\n",
        "        self._threshold = threshold\n",
        "\n",
        "    def set_node_value(self, node_value):  # To set node value if leaf node\n",
        "        self._node_value = node_value\n",
        "\n",
        "    def set_left_node(self, left):  # To set left node if not leaf node\n",
        "        self._left = left\n",
        "\n",
        "    def set_right_node(self, right):  # To set right node if not leaf node\n",
        "        self._right = right\n",
        "\n",
        "    def get_feature(self):  # Returns feature to divide with\n",
        "        return self._feature\n",
        "\n",
        "    def get_threshold(self):  # Returns threshold to divide with\n",
        "        return self._threshold\n",
        "\n",
        "    def get_node_value(self):  # Returns node value if leaf node\n",
        "        return self._node_value\n",
        "\n",
        "    def get_left_node(self):  # Returns left node if not leaf node\n",
        "        return self._left\n",
        "\n",
        "    def get_right_node(self):  # Returns right node if not leaf node\n",
        "        return self._right\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=10, n_features=None):  # Constructor\n",
        "        self._min_samples_split = min_samples_split\n",
        "        self._max_depth = max_depth\n",
        "        self._n_features = n_features\n",
        "        self._root = None\n",
        "\n",
        "    def fit(self, x, y):  # To fit the data to the tree\n",
        "        if self._n_features is None:  # if number of features not set then set it\n",
        "            self._n_features = x.shape[1]\n",
        "        else:\n",
        "            self._n_features = np.min(x.shape[1], self._n_features)\n",
        "        self._root = self._construct_tree(x, y)\n",
        "\n",
        "    def unique_labels(self, y):  # Finds and returns the unique labels given y labels\n",
        "        y = list(y)\n",
        "        label = y[0]\n",
        "        list_labels = []\n",
        "        list_labels.append(label)\n",
        "\n",
        "        for i in range(1, len(y), 1):\n",
        "            label = y[i]\n",
        "            if label in list_labels:\n",
        "                pass\n",
        "            else:\n",
        "                list_labels.append(label)\n",
        "        list_labels = np.array(list_labels)\n",
        "        list_labels.sort()\n",
        "        return list_labels\n",
        "\n",
        "    def random_feature_indexes(self, a, size):  # Finds randomly the indexes of features, given the number of indexes we\n",
        "        if size > a:                            # want to find\n",
        "            raise ValueError(str(size) + \" features can't be chosen out of \" + str(a))\n",
        "        random_feat_indexes = []\n",
        "        random_feat = random.randrange(0, a, 1)\n",
        "        random_feat_indexes.append(random_feat)\n",
        "\n",
        "        for _ in range(1, size, 1):\n",
        "            random_feat = random.randrange(0, a, 1)\n",
        "            while random_feat in random_feat_indexes:\n",
        "                random_feat = random.randrange(0, a, 1)\n",
        "            random_feat_indexes.append(random_feat)\n",
        "\n",
        "        random_feat_indexes = np.array(random_feat_indexes)\n",
        "        return random_feat_indexes\n",
        "\n",
        "    def _construct_tree(self, x, y, depth=0):  # Constructs the tree recursively\n",
        "        number_of_samples = x.shape[0]\n",
        "        number_of_features = x.shape[1]\n",
        "        number_of_labels = len(self.unique_labels(y))\n",
        "\n",
        "        # Stop constructing the tree if maximum depth is exceeded, then set the node value\n",
        "        if depth >= self._max_depth:\n",
        "            node_value = self._common_label(y)\n",
        "            node = Node()\n",
        "            node.set_node_value(node_value)\n",
        "            return node\n",
        "\n",
        "        if number_of_labels == 1:  # If only one label remains, set the node value\n",
        "            node_value = self._common_label(y)\n",
        "            node = Node()\n",
        "            node.set_node_value(node_value)\n",
        "            return node\n",
        "\n",
        "        if number_of_samples < self._min_samples_split:  # Stop if number of samples is less than the minimum number of\n",
        "            node_value = self._common_label(y)           # samples to split, then set the node value\n",
        "            node = Node()\n",
        "            node.set_node_value(node_value)\n",
        "            return node\n",
        "\n",
        "        feat_indexes = self.random_feature_indexes(number_of_features, self._n_features)  # Find random feature indexes\n",
        "        # Get the most suitable feature and threshold to divide with\n",
        "        best_feature, best_thresh = self._most_suitable_feature(x, y, feat_indexes)\n",
        "        # Construct the children\n",
        "        left_indexes, right_indexes = self._split(x[:, best_feature], best_thresh)\n",
        "        left_subtree = self._construct_tree(x[left_indexes, :], y[left_indexes], depth + 1)  # Left child\n",
        "        right_subtree = self._construct_tree(x[right_indexes, :], y[right_indexes], depth + 1)  # Right child\n",
        "        node = Node()  # Instantiate Node object\n",
        "        node.set_feature(best_feature)  # Set feature\n",
        "        node.set_threshold(best_thresh)  # Set threshold\n",
        "        node.set_left_node(left_subtree)  # Set left node\n",
        "        node.set_right_node(right_subtree)  # Set right node\n",
        "        return node\n",
        "\n",
        "    def _most_suitable_feature(self, x, y, feat_indexes):  # Finds the best feature and threshold to divide with\n",
        "        best_gain = -np.inf\n",
        "        split_feature_index, split_threshold = None, None\n",
        "        for feat_index in feat_indexes:\n",
        "            x_column = x[:, feat_index]\n",
        "            thresholds = self.unique_labels(x_column)\n",
        "            for threshold in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, x_column, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_feature_index = feat_index\n",
        "                    split_threshold = threshold\n",
        "        return split_feature_index, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, x_column, threshold):\n",
        "        # Entropy of the parent\n",
        "        parent_entropy = self._entropy(y)\n",
        "        # Creation of children\n",
        "        left_indexes, right_indexes = self._split(x_column, threshold)\n",
        "        if len(left_indexes) == 0:\n",
        "            return 0\n",
        "        if len(right_indexes) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Child node entropy\n",
        "        entropy_left = self._entropy(y[left_indexes])\n",
        "        entropy_right = self._entropy(y[right_indexes])\n",
        "        # Compute the weighted entropy of the child nodes\n",
        "        total_samples = len(y)\n",
        "        samples_left = len(left_indexes)\n",
        "        samples_right = len(right_indexes)\n",
        "        child_node_entropy = (samples_left / total_samples) * entropy_left + (samples_right / total_samples) * entropy_right\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_node_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def find_indexes(self, array, threshold):  # Find indexes of left and right\n",
        "        array = list(array)\n",
        "        left = []\n",
        "        right = []\n",
        "        for arr_elem in array:\n",
        "            if arr_elem <= threshold:\n",
        "                left.append(array.index(arr_elem))\n",
        "            if arr_elem > threshold:\n",
        "                right.append(array.index(arr_elem))\n",
        "\n",
        "        left = np.array(left)\n",
        "        right = np.array(right)\n",
        "        return left, right\n",
        "\n",
        "    def _split(self, x_column, split_thresh):  # Returns left and right indexes using method find_indexes\n",
        "        left_indexes, right_indexes = self.find_indexes(x_column, split_thresh)\n",
        "        return left_indexes, right_indexes\n",
        "\n",
        "    def _entropy(self, y):  # Computing the entropy\n",
        "        occurrences = []\n",
        "        uniq_labels = self.unique_labels(y)\n",
        "        for i in range(len(uniq_labels)):\n",
        "            value = uniq_labels[i]\n",
        "            num_occurrences = 0\n",
        "            for val in y:\n",
        "                if value == val:\n",
        "                    num_occurrences += 1\n",
        "            occurrences.append(num_occurrences)\n",
        "\n",
        "        occurrences = np.array(occurrences)\n",
        "        occur = []\n",
        "        for i in range(len(occurrences)):\n",
        "            nx = occurrences[i]\n",
        "            length = len(y)\n",
        "            px = nx / length\n",
        "            occur.append(px)\n",
        "\n",
        "        the_occurrences = []\n",
        "        for occ in occur:\n",
        "            if occ > 0:\n",
        "                the_occurrences.append(occ * np.log(occ))\n",
        "        the_occurrences = np.array(the_occurrences)\n",
        "        return -np.sum(the_occurrences)\n",
        "\n",
        "    def _majority_label(self, y):  # Given the y labels returns the one appearing more times\n",
        "        labels = self.unique_labels(y)\n",
        "        occurrences = []\n",
        "        majority_lab = y[0]\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            num_occur = 0\n",
        "            for lab in y:\n",
        "                if lab == label:\n",
        "                    num_occur += 1\n",
        "            occurrences.append(num_occur)\n",
        "\n",
        "        for i in range(len(occurrences)):\n",
        "            for n in range(len(occurrences)):\n",
        "                if i == n:\n",
        "                    pass\n",
        "                else:\n",
        "                    if occurrences[i] > occurrences[n]:\n",
        "                        majority_lab = labels[i]\n",
        "        return majority_lab\n",
        "\n",
        "    def _common_label(self, y):  # Returns the most common label using the _majority_label method\n",
        "        label = self._majority_label(y)\n",
        "        return label\n",
        "\n",
        "    def predict(self, samples):  # Given samples, for each sample traverse the tree starting at the root\n",
        "        preds = []\n",
        "        for sample in samples:\n",
        "            preds.append(self._search_tree(sample, self._root))\n",
        "        preds = np.array(preds)\n",
        "        return preds\n",
        "\n",
        "    def _search_tree(self, sample, node):  # Recursively traverse the tree given a sample starting at the root\n",
        "        node_value = node.get_node_value()\n",
        "        if node_value is not None:\n",
        "            return node_value\n",
        "        feature = node.get_feature()  # Feature to divide with\n",
        "        threshold = node.get_threshold()  # With which threshold to divide with\n",
        "        if sample[feature] <= threshold:\n",
        "            return self._search_tree(sample, node.get_left_node())\n",
        "        else:\n",
        "            return self._search_tree(sample, node.get_right_node())\n",
        "\n",
        "    def single_predict(self, sample):  # Prediction if we have one sample\n",
        "        prediction = self._search_tree(sample, self._root)\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "dSloQUGkyfN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Standard scaler and splitting of the data.\n",
        "- standard_scaler uses means and std deviations to standardize the features.\n",
        "- split splits the data into training and testing sets."
      ],
      "metadata": {
        "id": "s0hzsv4_zKWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(features):\n",
        "    # Computation of mean and standard deviation\n",
        "    means = features.mean(axis=0)\n",
        "    std_deviations = features.std(axis=0)\n",
        "    # Standardizing the features\n",
        "    standardized_features = (features - means) / std_deviations\n",
        "    standardized_features = np.array(standardized_features)\n",
        "    return standardized_features\n",
        "\n",
        "def _random_indexes(array, size, random_state):  # For selecting the indexes for test features\n",
        "    if size > array:\n",
        "        raise ValueError(str(size) + \" features can't be chosen out of \" + str(array))\n",
        "    random_indexes = []\n",
        "    random.seed(random_state)\n",
        "    random_index = random.randrange(0, array, 1)\n",
        "    random_indexes.append(random_index)\n",
        "\n",
        "    for _ in range(1, size, 1):\n",
        "        random_index = random.randrange(0, array, 1)\n",
        "        while random_index in random_indexes:\n",
        "            random_index = random.randrange(0, array, 1)\n",
        "        random_indexes.append(random_index)\n",
        "    random_indexes = np.array(random_indexes)\n",
        "    return random_indexes\n",
        "\n",
        "def split(features, targets, test_size, random_state=50):\n",
        "    number_of_samples = len(targets)\n",
        "    t_size = test_size * number_of_samples\n",
        "    t_size = int(t_size) + 1\n",
        "    random_indexes = _random_indexes(number_of_samples, t_size, random_state)\n",
        "    x_training, x_testing, y_training, y_testing = [], [], [], []\n",
        "    features = list(features)\n",
        "    targets = list(targets)\n",
        "\n",
        "    for i in range(len(random_indexes)):\n",
        "        x_testing.append(features[random_indexes[i]])\n",
        "        y_testing.append(targets[random_indexes[i]])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        if i in random_indexes:\n",
        "            pass\n",
        "        else:\n",
        "            x_training.append(features[i])\n",
        "            y_training.append(targets[i])\n",
        "\n",
        "    x_training, x_testing, y_training, y_testing = np.array(x_training), np.array(x_testing), np.array(y_training), \\\n",
        "        np.array(y_testing)\n",
        "    return x_training, x_testing, y_training, y_testing"
      ],
      "metadata": {
        "id": "hCTzemCLzTmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics\n",
        "- _confusion_matrix computes true positives, fales positives, true negatives, and false negatives.\n",
        "- accuracy_score, precision_score, recall_score, f1_score andd confusion_matrix compute accuracy, precison, recall, and f1 scores and confusion matrix."
      ],
      "metadata": {
        "id": "d6BDJkpTzZz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _confusion_matrix(y_testing, y_prediction):\n",
        "    # Computing confusion matrix\n",
        "    length_of_labels = len(y_testing)\n",
        "    true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
        "\n",
        "    for i in range(length_of_labels):\n",
        "        if y_testing[i] == 1:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_positive += 1\n",
        "\n",
        "        if y_testing[i] == 0:\n",
        "            if y_testing[i] == y_prediction[i]:\n",
        "                true_negative += 1\n",
        "            else:\n",
        "                false_negative += 1\n",
        "    return true_positive, false_positive, true_negative, false_negative\n",
        "\n",
        "def accuracy_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    return accuracy\n",
        "\n",
        "def precision_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    precision = tp / (tp + fp)\n",
        "    return precision\n",
        "\n",
        "def recall_score(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    tp_fn = tp + fn\n",
        "    if tp_fn == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        recall = tp / tp_fn\n",
        "        return recall\n",
        "\n",
        "def f1_score(y_testing, y_preds):\n",
        "    precision = precision_score(y_testing, y_preds)\n",
        "    recall = recall_score(y_testing, y_preds)\n",
        "    precision_recall = precision + recall\n",
        "    if precision_recall == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        f1 = (2 * precision * recall) / precision_recall\n",
        "        return f1\n",
        "\n",
        "def confusion_matrix(y_testing, y_preds):\n",
        "    tp, fp, tn, fn = _confusion_matrix(y_testing, y_preds)\n",
        "    con_mat = []\n",
        "    positives = [tp, fp]\n",
        "    negatives = [fn, tn]\n",
        "    con_mat.append(positives)\n",
        "    con_mat.append(negatives)\n",
        "    return con_mat"
      ],
      "metadata": {
        "id": "vmoWLkFCzgWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Main function."
      ],
      "metadata": {
        "id": "y0TpRcO_zzHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Ky-KjECObn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ab36fd-25b1-4ea2-cb85-fd8c06c941d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score on the validation data:  0.75\n",
            "Precision score on the validation data:  0.5\n",
            "Recall score on the validation data:  1.0\n",
            "F1 score on the validation data:  0.6666666666666666\n",
            "The confusion matrix on the validation:  [[3, 3], [0, 6]]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    feats, y_labels = spectral_centroid_rolloff()\n",
        "    features = standard_scaler(feats)\n",
        "    X_train, X_valid, y_train, y_valid = split(features, y_labels, test_size=0.20)\n",
        "    tree = DecisionTree(min_samples_split=1, max_depth=1)\n",
        "    tree.fit(X_train, y_train)\n",
        "    predictions = tree.predict(X_valid)\n",
        "\n",
        "    # Metrics on the validation data\n",
        "    accuracy = accuracy_score(y_valid, predictions)\n",
        "    precision = precision_score(y_valid, predictions)\n",
        "    recall = recall_score(y_valid, predictions)\n",
        "    f1 = f1_score(y_valid, predictions)\n",
        "    confusion_mat = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "    print('Accuracy score on the validation data: ', accuracy)\n",
        "    print('Precision score on the validation data: ', precision)\n",
        "    print('Recall score on the validation data: ', recall)\n",
        "    print('F1 score on the validation data: ', f1)\n",
        "    print('The confusion matrix on the validation: ', confusion_mat)\n",
        "    test = X_valid[5]\n",
        "\n",
        "    # # Saving the model using pickel\n",
        "    # tree_model = \"tree_model.pkl\"\n",
        "    # # with open(tree_model, 'wb') as file:\n",
        "    # #     pickle.dump(tree, file)\n",
        "\n",
        "    # with open(tree_model, 'rb') as file:\n",
        "    #     model = pickle.load(file)\n",
        "    # pred = model.single_predict(test)\n",
        "    # print(pred)\n",
        "    # # Saving the model using Joblib\n",
        "    # file_name = 'decision_tree_model.sav'\n",
        "    # # joblib.dump(tree, file_name)\n",
        "    # loaded_tree = joblib.load(file_name)\n",
        "    # y_pred = loaded_tree.single_predict(test)\n",
        "    # print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofDwd9fFlzh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}